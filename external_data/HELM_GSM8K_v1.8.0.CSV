Model,EM,Observed inference time (s),# eval,# train,truncated,# prompt tokens,# output tokens
Llama 3.1 Instruct Turbo (405B),0.949,2.737,1000,5,0,959.032,122.777
Claude 3.5 Sonnet (20240620),0.949,3.163,1000,5,0,938.712,165.163
Llama 3.1 Instruct Turbo (70B),0.938,4.99,1000,5,0,959.032,127.086
GPT-4 (0613),0.932,4.948,1000,5,0,1020.035,111.209
Claude 3 Opus (20240229),0.924,7.469,1000,5,0,1012.712,115.934
Qwen2 Instruct (72B),0.92,6.592,1000,5,0,1130.403,166.4
Mistral Large 2 (2407),0.912,5.431,1000,5,0,1187.268,205.748
Claude 3 Sonnet (20240229),0.907,3.213,1000,5,0,1012.712,114.663
GPT-4o (2024-05-13),0.905,4.227,1000,5,0,952.617,213.475
Palmyra-X-004,0.905,11.45,1000,5,0,959.032,174.327
Phi-3 (14B),0.878,74.933,1000,5,0,1207.746,400
Jamba 1.5 Large,0.846,3.942,1000,5,0,1163.818,0
GPT-4o mini (2024-07-18),0.843,2.519,1000,5,0,952.617,215.465
Gemini 1.5 Pro (001),0.836,3.206,1000,5,0,1151.885,0
PaLM-2 (Unicorn),0.831,5.437,1000,5,0,1109.549,93.764
Palmyra X V3 (72B),0.831,5.07,1000,5,0,938.869,89.919
GPT-4 Turbo (2024-04-09),0.824,6.915,1000,5,0,959.035,141.712
Gemini 1.0 Pro (002),0.816,1.513,1000,5,0,1151.885,0
Qwen1.5 Chat (110B),0.815,4.537,1000,5,0,1130.403,175.784
Gemma 2 Instruct (27B),0.812,2.332,1000,5,0,1151.885,1
Llama 3 (70B),0.805,4.2,1000,5,0,959.032,1
Mixtral (8x22B),0.8,3.539,1000,5,0,1187.268,1
Qwen1.5 (72B),0.799,4.587,1000,5,0,1130.403,1
Llama 3.1 Instruct Turbo (8B),0.798,2.109,1000,5,0,959.032,150.02
DeepSeek LLM Chat (67B),0.795,5.877,1000,5,0,1233.708,1
Gemini 1.5 Flash (001),0.785,1.758,1000,5,0,1151.885,0
Claude v1.3,0.784,6.653,1000,5,0,1012.712,104.726
Mistral NeMo (2402),0.782,1.425,1000,5,0,1134.356,187.859
Qwen1.5 (32B),0.773,3.406,1000,5,0,1130.403,1
Arctic Instruct,0.768,2.961,1000,5,0,1207.746,189.305
Gemma 2 Instruct (9B),0.762,1.72,1000,5,0,1151.885,1
Command R Plus,0.738,3.592,1000,5,0,1158.893,0
Palmyra X V2 (33B),0.735,2.543,1000,5,0,938.869,89.718
Mistral Small (2402),0.734,2.972,1000,5,0,1255.268,148.06
Claude Instant 1.2,0.721,1.474,1000,5,0,1012.712,105.998
Mistral Medium (2312),0.706,9.719,1000,5,0,1255.268,137.554
Claude 3 Haiku (20240307),0.699,1.228,1000,5,0,1012.712,77.518
Mistral Large (2402),0.694,7.095,1000,5,0,1255.268,129.185
Qwen1.5 (14B),0.693,1.966,1000,5,0,1130.403,1
Jamba 1.5 Mini,0.691,1.892,1000,5,0,1163.818,0
Yi Large (Preview),0.69,13.45,1000,5,0,1170.814,288.079
DBRX Instruct,0.671,2.384,1000,5,0,1020.035,1
Jamba Instruct,0.67,3.846,1000,5,0,823.394,0
GPT-4 Turbo (1106 preview),0.668,5.738,1000,5,0,1020.035,98.073
Yi (34B),0.648,4.887,1000,5,0,1170.814,1
Mixtral (8x7B 32K seqlen),0.622,3.273,1000,5,0,1187.268,1
GPT-3.5 (text-davinci-003),0.615,5.199,1000,5,0,938.869,93.717
PaLM-2 (Bison),0.61,1.44,1000,5,0,1109.549,94.258
Claude 2.1,0.604,7.706,1000,5,0,1012.712,98.553
Qwen1.5 (7B),0.6,1.381,1000,5,0,1130.403,1
Claude 2.0,0.583,4.857,1000,5,0,1012.712,78.704
Phi-2,0.581,1.147,1000,5,0,938.893,1
Llama 2 (70B),0.567,3.737,1000,5,0,1207.746,1
Gemma (7B),0.559,2.025,1000,5,0,1151.885,1
Command R,0.551,1.04,1000,5,0,1158.893,0
Mistral Instruct v0.3 (7B),0.538,3.95,1000,5,0,1187.268,196.611
GPT-3.5 Turbo (0613),0.501,0.898,1000,5,0,1020.035,77.29
Llama 3 (8B),0.499,1.771,1000,5,0,959.032,1
LLaMA (65B),0.489,12.339,1000,5,0,1207.746,1
GPT-3.5 (text-davinci-002),0.479,3.762,1000,5,0,938.869,90.543
Command,0.452,4.127,1000,5,0,942.424,94.43
Mistral v0.1 (7B),0.377,1.632,1000,5,0,1187.268,1
Yi (6B),0.375,1.878,1000,5,0,1170.814,1
Falcon (40B),0.267,12.967,1000,5,0,1056.967,1
Llama 2 (13B),0.266,1.737,1000,5,0,1207.746,1
Jurassic-2 Jumbo (178B),0.239,5.176,1000,5,0,823.394,102.036
Jurassic-2 Grande (17B),0.159,5.417,1000,5,0,823.394,121.336
Llama 2 (7B),0.154,1.96,1000,5,0,1207.746,1
Command Light,0.149,1.751,1000,5,0,942.424,80.184
Luminous Supreme (70B),0.137,48.242,1000,5,0,943.121,400
Luminous Extended (30B),0.075,22.685,1000,5,0,943.121,400
Falcon (7B),0.055,6.94,1000,5,0,1056.967,1
OLMo (7B),0.044,2.41,1000,5,0,939.582,1
Luminous Base (13B),0.028,16.427,1000,5,0,943.121,400