Model,EM,Observed inference time (s),# eval,# train,truncated,# prompt tokens,# output tokens
Claude 3.5 Sonnet (20240620),0.865,1.1,246.351,5,0,647.288,1
Claude 3 Opus (20240229),0.846,4.077,246.351,5,0,638.288,1
Llama 3.1 Instruct Turbo (405B),0.845,0.685,246.351,5,0,614.619,1
GPT-4o (2024-05-13),0.842,0.37,246.351,5,0,612.332,1
Gemini 1.5 Pro (001),0.827,0.618,246.351,5,0,632.617,0
GPT-4 (0613),0.824,0.447,246.351,5,0,607.852,1
Qwen2 Instruct (72B),0.824,0.359,246.351,5,0,625.598,1
Palmyra-X-004,0.813,0.535,246.351,5,0,614.619,0.991
GPT-4 Turbo (2024-04-09),0.813,0.617,246.351,5,0,614.852,1
Gemini 1.5 Pro (0409 preview),0.81,1.174,246.351,5,0,632.617,0
Llama 3.1 Instruct Turbo (70B),0.801,5.993,246.351,5,0,614.619,1
Mistral Large 2 (2407),0.8,0.798,246.351,5,0,706.273,1
GPT-4 Turbo (1106 preview),0.796,0.537,246.351,5,0,607.852,1
Llama 3 (70B),0.793,0.462,246.351,5,0,607.619,1
Yi Large (Preview),0.793,0.764,246.351,5,0,674.842,1
Palmyra X V3 (72B),0.786,0.663,246.351,5,0,627.489,1
PaLM-2 (Unicorn),0.786,1.052,246.351,5,0,635.61,0
Jamba 1.5 Large,0.782,1.01,246.351,5,0,658.432,0
Gemini 1.5 Flash (001),0.779,0.487,246.351,5,0,632.617,0
Mixtral (8x22B),0.778,0.555,246.351,5,0,696.273,1
Gemini 1.5 Flash (0514 preview),0.778,0.348,246.351,5,0,632.617,0
Phi-3 (14B),0.775,4.948,246.351,5,0,714.893,1
Qwen1.5 (72B),0.774,0.375,246.351,5,0,618.598,1
Qwen1.5 Chat (110B),0.768,0.287,246.351,5,0,625.598,1
GPT-4o mini (2024-07-18),0.767,0.334,246.351,5,0,612.332,1
Yi (34B),0.762,0.823,246.351,5,0,661.842,1
Claude 3 Sonnet (20240229),0.759,1.468,246.351,5,0,638.288,1
Gemma 2 (27B),0.757,2.744,246.351,5,0,624.617,1
Phi-3 (7B),0.757,0.38,246.351,5,0,614.852,1
Qwen1.5 (32B),0.744,0.413,246.351,5,0,618.598,1
DBRX Instruct,0.741,0.459,246.351,5,0,607.852,1
Claude 3 Haiku (20240307),0.738,0.734,246.351,5,0,638.288,1
Claude 2.1,0.735,2.418,246.351,5,0,703.288,1
DeepSeek LLM Chat (67B),0.725,0.591,246.351,5,0,644.941,1
Gemma 2 (9B),0.721,0.901,246.351,5,0,624.617,1
Mixtral (8x7B 32K seqlen),0.717,0.364,246.351,5,0,696.273,1
Gemini 1.0 Pro (001),0.7,0.385,246.351,5,0,624.617,0
Jamba 1.5 Mini,0.695,0.858,236.768,5,0,664.257,0
Llama 2 (70B),0.695,0.466,246.351,5,0,706.682,1
Command R Plus,0.694,0.305,246.351,5,0,648.571,0
PaLM-2 (Bison),0.692,1.845,246.351,5,0,635.61,1
GPT-3.5 Turbo (0613),0.689,0.411,246.351,5,0,607.852,1
Claude Instant 1.2,0.688,0.932,246.351,5,0,703.288,1
Mistral Large (2402),0.688,0.546,246.351,5,0,696.273,1
Mistral Small (2402),0.687,0.486,246.351,5,0,696.273,1
Qwen1.5 (14B),0.686,0.321,246.351,5,0,618.598,1
Arctic Instruct,0.677,0.42,246.351,5,0,706.682,1
Llama 3 (8B),0.668,0.35,246.351,5,0,607.619,1
Gemma (7B),0.661,0.312,246.351,5,0,624.617,1
Jamba Instruct,0.659,0.277,246.351,5,0,490.686,0
Mistral NeMo (2402),0.653,0.852,246.351,5,0,627.375,1
Command R,0.652,0.176,246.351,5,0,648.571,0
Yi (6B),0.64,0.388,246.351,5,0,661.842,1
Qwen1.5 (7B),0.626,0.302,246.351,5,0,618.598,1
Mistral Instruct v0.3 (7B),0.599,0.526,246.351,5,0,705.273,1
Phi-2,0.584,0.309,246.351,4.946,0,600.9,1
Mistral v0.1 (7B),0.566,0.864,246.351,5,0,696.273,1
Llama 3.1 Instruct Turbo (8B),0.561,0.56,246.351,5,0,614.619,1
Llama 2 (13B),0.554,0.492,246.351,5,0,706.682,1
OLMo 1.7 (7B),0.538,1.024,246.351,4.946,0,597.916,1
Llama 2 (7B),0.458,0.374,246.351,5,0,706.682,1
OLMo (7B),0.295,0.386,246.351,4.946,0,597.867,1
