{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all code necessary to run the experiments and produce the graphs in the paper \"Conditional and Modal Reasoning in Large Language Models\" (https://arxiv.org/abs/2401.17169) by Wesley H. Holliday (wesholliday@berkeley.edu) and Matthew Mandelkern (mandelkern@nyu.edu).\n",
    "\n",
    "The notebook was created using Python 3.11.6 and Seaborn 0.12.0.\n",
    "\n",
    "To collect responses from one of the open-source LLMs, use LM Studio (https://lmstudio.ai), v0.2.11 or later, to set up a local inference server with base_url=http://localhost:1234/v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "### 1. Setup\n",
    "\n",
    "### 2. Collect model response data\n",
    "\n",
    "### 3. Create dataframe\n",
    "\n",
    "### 4. Create figures for paper\n",
    "\n",
    "### 5. Further analysis mentioned in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict ={0: (\"code_llama_7B\", \"Code Llama 7B\", \"code llama instruct 7B Q6_K gguf\"),\n",
    "             1: (\"code_llama_13B\", \"Code Llama 13B\", \"code llama instruct 13B Q6_K gguf\"),\n",
    "             2: (\"code_llama_34B\", \"Code Llama 34B\", \"code llama instruct 34B Q6_K gguf\"),\n",
    "             3: (\"gpt_3_5_turbo_0613\", \"GPT-3.5 Turbo (0613)\", \"gpt-3.5-turbo-0613\"), # This is the model that 'gpt-3.5-turbo' points to in the Open AI API as of January 24, 2024\n",
    "             #4: (\"gpt_3_5_turbo_1106\", \"GPT-3.5 Turbo (1106)\", \"gpt-3.5-turbo-1106\"), # We also tested this model, but it is not included in the paper\n",
    "             #5: (\"gpt_4_0314\", \"GPT-4 (0314)\", \"gpt-4-0314\"), # We also tested this model, but it is not included in the paper\n",
    "             6: (\"gpt_4_0613\", \"GPT-4 (0613)\", \"gpt-4-0613\"), # This is the model that 'gpt-4' points to in the Open AI API as of January 24, 2024\n",
    "             #7: (\"gpt_4_turbo_1106\", \"GPT-4 Turbo\", \"gpt-4-1106-preview\"), # We also tested this model, but it is not included in the paper except in a footnote\n",
    "             8: (\"llama_2_chat_7B\", \"Llama 2 Chat 7B\", \"llama 2 chat 7B Q6_K gguf\"),\n",
    "             9: (\"llama_2_chat_13B\", \"Llama 2 Chat 13B\", \"llama 2 chat 13B Q6_K gguf\"), \n",
    "             10: (\"llama_2_chat_70B\", \"Llama 2 Chat 70B\", \"llama 2 chat 70B Q6_K gguf\"),\n",
    "             11: (\"mistral_instruct\", \"Mistral 7B\", \"mistral instruct v0 2 7B Q6_K gguf\"), \n",
    "             12: (\"mixtral_instruct\", \"Mixtral 8x7B\", \"mixtral 8x instruct v0 1 7B Q6_K gguf\"), \n",
    "             13: (\"phi_2\", \"Phi-2\", \"phi 2 3B Q6_K gguf\"), \n",
    "             14: (\"yi_chat_34B\", \"Yi Chat 34B\", \"yi chat 34B Q6_K gguf\"), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 0 # This is the model that will be used in Section 2\n",
    "model_short_name = model_dict[model_num][0]\n",
    "model_display_name = model_dict[model_num][1]\n",
    "model_full_name = model_dict[model_num][2]\n",
    "print(model_short_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_prompt_classes = ['MT','MTx','uMT','vMT','MTo','MTox', # versions of Modus Tollens\n",
    "                           'MP','MPx','uMP','vMP','MPo','MPox', # versions of Modus Ponens\n",
    "\n",
    "                           'AC','ACx','uAC','vAC','ACo','ACox', # versions of Affirming the Consequent\n",
    "                           'LAC','cAC','LcAC','dAC','LdAC','sAC','snAC','gAC','gnAC', # more versions of Affirming the Consequent with altered phrasing\n",
    "                           'CONV','CONVx','uCONV','vCONV', # versions of Conversion\n",
    "\n",
    "                           'DA','DAx','uDA','vDA','DAo','DAox', # versions of Denying the Antecedent\n",
    "                           'INV','INVx','uINV','vINV', # versions of Inversion\n",
    "\n",
    "                           'DS','DSx','uDS','vDS','DSo','DSox','DSf','DSfx','DSfo','DSfox', # versions of Disjunctive Syllogism   \n",
    "                           \n",
    "                           'CT','CTx','CTn','CTnx','CTnd','uCTnd','vCTnd', # versions of Contraposition\n",
    "                           'ASd','ASx','uASd','vASd', # versions of Antecedent Strengthening\n",
    "\n",
    "                           'MiN','MiNx','uMiN','vMiN','NMu','NMux','uNMu','vNMu', # versions of Duality inferences \n",
    "\n",
    "                           'MTmu','MTmux','uMTmu','vMTmu','MTmuo','MTmuox', # versions of Modus Tollens Must\n",
    "                           'MTmi','MTmix','uMTmi','vMTmi','MTmio','MTmiox', # versions of Modus Tollens Might\n",
    "\n",
    "                           'DSmu','DSmux','uDSmu','vDSmu','DSmuo','DSmuox','DSmuf','DSmufx','DSmufo','DSmufox', # versions of Disjunctive Syllogism Must\n",
    "                           'DSmi','DSmix','uDSmi','vDSmi','DSmio','DSmiox','DSmif','DSmifx','DSmifo','DSmifox', # versions of Disjunctive Syllogism Might\n",
    "]\n",
    "\n",
    "user_prompts = list()\n",
    "prompt_classes = list()\n",
    "\n",
    "for file in os.listdir('prompts'):\n",
    "    if file.endswith('.json'):\n",
    "        prompt_class = file[:-5]\n",
    "        if prompt_class in selected_prompt_classes:\n",
    "            prompt_classes.append(file[:-5])\n",
    "            with open('prompts/' + file) as f:\n",
    "                user_prompts += json.load(f)\n",
    "        else:\n",
    "            print('Skipping', file[:-5])\n",
    "\n",
    "print(sorted(prompt_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Answer only with 'yes' or 'no' and nothing else.\"\n",
    "\n",
    "temperatures = [0, 0.5, 1]\n",
    "\n",
    "num_prompts_per_temp = {0: 1, 0.5: 20, 1: 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "A number of the LLMs often disobey the system prompt to say *nothing else* except 'yes' or 'no'. Thus, we extract 'yes' or 'no' from their response using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_yes(response):\n",
    "    # Regular expression pattern\n",
    "    # \\b represents a word boundary\n",
    "    # 'yes' is the word to search\n",
    "    # [.,;:-]? allows for an optional period, comma, semicolon, colon, or dash after \"yes\"\n",
    "    # re.IGNORECASE makes the search case-insensitive\n",
    "    pattern = r'\\byes\\b[.,;:-]?'\n",
    "\n",
    "    # Search for the pattern in the response\n",
    "    return re.search(pattern, response, re.IGNORECASE) is not None\n",
    "\n",
    "def contains_no(response):\n",
    "    # Regular expression pattern\n",
    "    # \\b represents a word boundary\n",
    "    # 'no' is the word to search\n",
    "    # [.,;:-]? allows for an optional period, comma, semicolon, colon, or dash after \"no\"\n",
    "    # re.IGNORECASE makes the search case-insensitive\n",
    "    pattern = r'\\bno\\b[.,;:-]?'\n",
    "\n",
    "    # Search for the pattern in the response\n",
    "    return re.search(pattern, response, re.IGNORECASE) is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collect model response data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in not only whether an LLM operating at zero temperature gives an affirmative answer or negative answer to a logical inference question, but also the frequency with which an LLM operating at non-zero temperature gives an affirmative answer or negative answer to a logical inference question. Elsewhere we are doing an analysis with log probabilities, but here we take a frequentist approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_responses = False # Set to True to get responses from the API. Set to False to just analyze the data that has already been collected.\n",
    "\n",
    "if get_responses:\n",
    "    print(\"Model:\", model_short_name)\n",
    "    print(\"\")\n",
    "\n",
    "    for up in tqdm(user_prompts):\n",
    "        prompt_class, prompt_num, user_prompt = up\n",
    "\n",
    "        for temperature in temperatures:\n",
    "\n",
    "            # if the data file already exists, continue\n",
    "            if os.path.exists(f'data/{model_short_name}/{prompt_class}/{model_short_name}_{prompt_class}_{prompt_num}_{temperature}.json'):\n",
    "                continue\n",
    "\n",
    "            print(\"Prompt:\", user_prompt)\n",
    "            print(\"Temperature:\", temperature)\n",
    "            print(\"\")\n",
    "\n",
    "            data = {\n",
    "                \"user_prompt\": user_prompt,\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"model\": model_full_name,\n",
    "                \"temperature\": temperature,\n",
    "                \"responses\": []\n",
    "            }\n",
    "\n",
    "            for n in range(num_prompts_per_temp[temperature]):\n",
    "\n",
    "                if model_short_name in ['gpt_4_turbo_1106', 'gpt_4_0613', 'gpt_4_0314', 'gpt_3_5_turbo_1106', 'gpt_3_5_turbo_0613']:\n",
    "                    \n",
    "                    client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "                    completion = client.chat.completions.create(\n",
    "                    model=model_full_name,\n",
    "                    messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=4,\n",
    "                )\n",
    "                \n",
    "                else:\n",
    "                    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\") \n",
    "                    completion = client.chat.completions.create(\n",
    "                    model=\"local-model\",\n",
    "                    messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=8,\n",
    "                )\n",
    "\n",
    "                print(completion.choices[0].message)\n",
    "\n",
    "                # get a JSON representation of the message\n",
    "                message_json = completion.choices[0].message.model_dump_json()\n",
    "                data[\"responses\"].append(json.loads(message_json))\n",
    "\n",
    "                # if all the responses are 'yes' or all response are 'no' by some checkpoint number of iterations, then break\n",
    "                if model_short_name in ['gpt_4_turbo_1106', 'gpt_4_0613', 'gpt_4_0314']:\n",
    "                    impatience = 4\n",
    "                else:\n",
    "                    impatience = 2\n",
    "\n",
    "                if n == (num_prompts_per_temp[temperature] // impatience) - 1:\n",
    "                    if all(contains_yes(response) for response in [data[\"responses\"][k][\"content\"] for k in range(n)]) and not any(contains_no(response) for response in [data[\"responses\"][k][\"content\"] for k in range(n)]):\n",
    "                        break\n",
    "                    elif all(contains_no(response) for response in [data[\"responses\"][k][\"content\"] for k in range(n)]) and not any(contains_yes(response) for response in [data[\"responses\"][k][\"content\"] for k in range(n)]):\n",
    "                        break\n",
    "\n",
    "            print(\"\")\n",
    "            \n",
    "            # create the appropriate directory if it doesn't already exist\n",
    "            if not os.path.exists(f'data/{model_short_name}/{prompt_class}'):\n",
    "                os.makedirs(f'data/{model_short_name}/{prompt_class}')\n",
    "\n",
    "            # Save the data to a JSON file\n",
    "            with open(f'data/{model_short_name}/{prompt_class}/{model_short_name}_{prompt_class}_{prompt_num}_{temperature}.json', 'w') as json_file:\n",
    "                json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional helper functions\n",
    "\n",
    "We noticed that models sometimes responded with 'we can' instead of 'yes' and 'we cannot' instead of 'no', so we allow these responses (but flag them below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_can(response):\n",
    "    # Regular expression pattern\n",
    "    # \\b represents a word boundary\n",
    "    # 'can' is the word to search\n",
    "    # (?!not\\b|'t\\b| be\\b) is a negative lookahead that ensures 'can' is not followed by 'not', 't', or ' be'\n",
    "    # [.,;:-!]? allows for an optional period, comma, semicolon, colon, or dash after \"can\"\n",
    "    # re.IGNORECASE makes the search case-insensitive\n",
    "    pattern = r'\\bcan\\b(?!not\\b|\\'t\\b| be\\b)[.,;:-]?'\n",
    "\n",
    "    # Search for the pattern in the response\n",
    "    return re.search(pattern, response, re.IGNORECASE) is not None\n",
    "\n",
    "def contains_cannot(response):\n",
    "    # Regular expression pattern\n",
    "    # \\b represents a word boundary\n",
    "    # '(?:cannot|can't)' is a non-capturing group that matches either 'cannot' or 'can't'\n",
    "    # [.,;:-!]? allows for an optional period, comma, semicolon, colon, or dash after \"cannot\" or \"can't\"\n",
    "    # re.IGNORECASE makes the search case-insensitive\n",
    "    pattern = r'\\b(?:cannot|can\\'t)\\b[.,;:-]?'\n",
    "\n",
    "    # Search for the pattern in the response\n",
    "    return re.search(pattern, response, re.IGNORECASE) is not None\n",
    "\n",
    "print(contains_can(\"We can infer that\"))\n",
    "print(contains_can(\"We can't infer that\"))\n",
    "print(contains_can(\"We cannot infer that\"))\n",
    "print(contains_can(\"The argument's structure can be represented...\"))\n",
    "print(\"\")\n",
    "print(contains_cannot(\"We can infer that\"))\n",
    "print(contains_cannot(\"We can't infer that\"))\n",
    "print(contains_cannot(\"We cannot infer that\"))\n",
    "print(contains_cannot(\"The argument's structure can be represented...\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data to create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False # Set to True to print out warnings and errors\n",
    "\n",
    "df = pd.DataFrame(columns=[\"prompt_class\", \"prompt_num\", \"prompt\", \"model\", \"temperature\", \"yes_count\", \"no_count\", \"num_trials\", \"yes_percent\", \"no_percent\"])\n",
    "\n",
    "model_display_names = [model_dict[mod][1] for mod in model_dict.keys()]\n",
    "queries_per_model = {model_display_name: 0 for model_display_name in model_display_names}\n",
    "\n",
    "error_both_response = {model_display_name: 0 for model_display_name in model_display_names}\n",
    "error_no_response = {model_display_name: 0 for model_display_name in model_display_names}\n",
    "\n",
    "can_or_cannot_response = {model_display_name: 0 for model_display_name in model_display_names}\n",
    "can_and_cannot_response = {model_display_name: 0 for model_display_name in model_display_names}\n",
    "\n",
    "for up in tqdm(user_prompts):\n",
    "    prompt_class, prompt_num, user_prompt = up\n",
    "    for mod in model_dict.keys():\n",
    "        model_short_name = model_dict[mod][0]\n",
    "        model_display_name = model_dict[mod][1]\n",
    "        for temperature in temperatures:\n",
    "            with open(f'data/{model_short_name}/{prompt_class}/{model_short_name}_{prompt_class}_{prompt_num}_{temperature}.json', 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                    \n",
    "                # count the number of times the models response contained \"yes\" or \"no\" (case insensitive)\n",
    "                yes_count = 0\n",
    "                no_count = 0\n",
    "\n",
    "                for response in data[\"responses\"]:\n",
    "                    \n",
    "                    queries_per_model[model_display_name] += 1\n",
    "\n",
    "                    if contains_yes(response[\"content\"]) and not contains_no(response[\"content\"]):\n",
    "                        yes_count += 1\n",
    "                    elif contains_no(response[\"content\"]) and not contains_yes(response[\"content\"]):\n",
    "                        no_count += 1\n",
    "                        \n",
    "                    elif contains_yes(response[\"content\"]) and contains_no(response[\"content\"]):\n",
    "                        error_both_response[model_display_name] += 1\n",
    "\n",
    "                        if verbose:\n",
    "                            print(\"ERROR: response contains both 'yes' and 'no'.\")\n",
    "                            print(\"Model:\", model_display_name)\n",
    "                            print(\"Temperature:\", temperature)\n",
    "                            print(\"Prompt:\", user_prompt)\n",
    "                            print(\"Response:\", response[\"content\"])\n",
    "                            print(\"\")\n",
    "\n",
    "                    elif contains_can(response[\"content\"]) and not contains_cannot(response[\"content\"]):\n",
    "                        yes_count += 1\n",
    "                        can_or_cannot_response[model_display_name] += 1\n",
    "\n",
    "                        if verbose:\n",
    "                            print(\"WARNING: response contains 'can' instead of 'yes'.\")\n",
    "                            print(\"Model:\", model_display_name)\n",
    "                            print(\"Temperature:\", temperature)\n",
    "                            print(\"Prompt:\", user_prompt)\n",
    "                            print(\"Response:\", response[\"content\"])\n",
    "                            print(\"\")\n",
    "\n",
    "                    elif contains_cannot(response[\"content\"]) and not contains_can(response[\"content\"]):\n",
    "                        no_count += 1\n",
    "                        can_or_cannot_response[model_display_name] += 1\n",
    "\n",
    "                        if verbose:\n",
    "                            print(\"WARNING: response contains 'cannot' instead of 'no'.\")\n",
    "                            print(\"Model:\", model_display_name)\n",
    "                            print(\"Temperature:\", temperature)\n",
    "                            print(\"Prompt:\", user_prompt)\n",
    "                            print(\"Response:\", response[\"content\"])\n",
    "                            print(\"\")\n",
    "\n",
    "                    elif contains_can(response[\"content\"]) and contains_cannot(response[\"content\"]):\n",
    "                        error_both_response[model_display_name] += 1\n",
    "                        can_and_cannot_response[model_display_name] += 1\n",
    "\n",
    "                        if verbose:\n",
    "                            print(\"ERROR: response contains both 'can' and 'cannot'.\")\n",
    "                            print(\"Model:\", model_display_name)\n",
    "                            print(\"Temperature:\", temperature)\n",
    "                            print(\"Prompt:\", user_prompt)\n",
    "                            print(\"Response:\", response[\"content\"])\n",
    "                            print(\"\")\n",
    "\n",
    "                    else:\n",
    "                        error_no_response[model_display_name] += 1\n",
    "\n",
    "                        if verbose:\n",
    "                            print(\"ERROR: could not extract an answer.\")\n",
    "                            print(\"Model:\", model_display_name)\n",
    "                            print(\"Temperature:\", temperature)\n",
    "                            print(\"Prompt:\", user_prompt)\n",
    "                            print(\"Response:\", response[\"content\"])\n",
    "                            print(\"\")               \n",
    "\n",
    "                # append the data to the dataframe\n",
    "                new_row = {\"prompt_class\": prompt_class, \n",
    "                           \"prompt_num\": prompt_num, \n",
    "                           \"prompt\": user_prompt, \n",
    "                           \"model\": model_display_name, \n",
    "                           \"temperature\": temperature, \n",
    "                           \"yes_count\": yes_count, \n",
    "                           \"no_count\": no_count, \n",
    "                           \"num_trials\": len(data[\"responses\"]), \n",
    "                           \"yes_percent\": yes_count/len(data[\"responses\"]), \n",
    "                           \"no_percent\": - no_count/len(data[\"responses\"])\n",
    "                           }\n",
    "                \n",
    "                new_row_df = pd.DataFrame([new_row])\n",
    "                df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df.to_csv(\"results/results.csv\")\n",
    "\n",
    "# save the dataframe as a json file\n",
    "df.to_json(\"results/results.json\", orient=\"records\")\n",
    "\n",
    "print(\"Total number of queries:\", queries_per_model)\n",
    "print(\"Both responses errors:\", error_both_response)\n",
    "print(\"Neither response errors:\", error_no_response)\n",
    "print(\"Can or cannot response:\", can_or_cannot_response)\n",
    "print(\"Can and cannot response:\", can_and_cannot_response)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create figures for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the correct answers (counting the expert human judgments from the cited literature as correct in the modal cases)\n",
    "correct_answers = {\n",
    "    'AC': 'no',\n",
    "    'ACo': 'no',\n",
    "    'ACox': 'no',\n",
    "    'ACx': 'no',\n",
    "    'ASd': 'no',\n",
    "    'ASx': 'no',\n",
    "    'cAC': 'no',\n",
    "    'CONV': 'no',\n",
    "    'CONVx': 'no',\n",
    "    'CT': 'no', # According to the modal analysis of conditionals, CT is not a valid form of inference, but the instances we tested are reasonable inferences\n",
    "    'CTx': 'no',\n",
    "    'CTn': 'no', # According to the modal analysis of conditionals, CTn is not a valid form of inference, but the instances we tested are reasonable inferences\n",
    "    'CTnd': 'no',\n",
    "    'CTnx': 'no',\n",
    "    'DA': 'no',\n",
    "    'dAC': 'no',\n",
    "    'DAx': 'no',\n",
    "    'DAo': 'no',\n",
    "    'DAox': 'no',\n",
    "    'uAC': 'no',\n",
    "    'uDA': 'no',\n",
    "    'DS': 'yes',\n",
    "    'DSf': 'yes',\n",
    "    'DSfo': 'yes',\n",
    "    'DSfox': 'yes',\n",
    "    'DSfx': 'yes',\n",
    "    'DSmi': 'no',\n",
    "    'DSmif': 'no',\n",
    "    'DSmifo': 'no',\n",
    "    'DSmifox': 'no',\n",
    "    'DSmifx': 'no',\n",
    "    'DSmio': 'no',\n",
    "    'DSmiox': 'no',\n",
    "    'DSmix': 'no',\n",
    "    'DSmu': 'no',\n",
    "    'DSmuf': 'no',\n",
    "    'DSmufo': 'no',\n",
    "    'DSmufx': 'no',\n",
    "    'DSmufox': 'no',\n",
    "    'DSmuo': 'no',\n",
    "    'DSmuox': 'no',\n",
    "    'DSmux': 'no',\n",
    "    'DSo': 'yes',\n",
    "    'DSox': 'yes',\n",
    "    'DSx': 'yes',\n",
    "    'gAC': 'no',\n",
    "    'gnAC': 'no',\n",
    "    'INV': 'no',\n",
    "    'INVx': 'no',\n",
    "    'LAC': 'no',\n",
    "    'LcAC': 'no',\n",
    "    'LdAC': 'no',\n",
    "    'MiN': 'yes',\n",
    "    'MiNx': 'yes',\n",
    "    'MP': 'yes',\n",
    "    'MPo': 'yes',\n",
    "    'MPox': 'yes',\n",
    "    'MPx': 'yes',\n",
    "    'MT': 'yes',\n",
    "    'MTmi': 'no',\n",
    "    'MTmio': 'no',\n",
    "    'MTmiox': 'no',\n",
    "    'MTmix': 'no',\n",
    "    'MTmu': 'no',\n",
    "    'MTmuo': 'no',\n",
    "    'MTmuox': 'no',\n",
    "    'MTmux': 'no',\n",
    "    'MTo': 'yes',\n",
    "    'MTox': 'yes',\n",
    "    'MTx': 'yes',\n",
    "    'mvMiN': 'yes',\n",
    "    'mvNmu': 'yes',\n",
    "    'NMu': 'yes',\n",
    "    'NMux': 'yes',\n",
    "    'sAC': 'no',\n",
    "    'snAC': 'no',\n",
    "    'uAC': 'no',\n",
    "    'uASd': 'no',\n",
    "    'uCONV': 'no',\n",
    "    'uDA': 'no',\n",
    "    'uDS': 'yes',\n",
    "    'uDSmi': 'no',\n",
    "    'uDSmu': 'no',\n",
    "    'uINV': 'no',\n",
    "    'uMiN': 'yes',\n",
    "    'uMP': 'yes',\n",
    "    'uMT': 'yes',\n",
    "    'uMTmi': 'no',\n",
    "    'uMTmu': 'no',\n",
    "    'uNMu': 'yes',\n",
    "    'vAC': 'no',\n",
    "    'vASd': 'no',\n",
    "    'vCONV': 'no',\n",
    "    'vCTnd': 'no',\n",
    "    'vDA': 'no',\n",
    "    'vDS': 'yes',\n",
    "    'vDSmi': 'no',\n",
    "    'vDSmu': 'no',\n",
    "    'vINV': 'no',\n",
    "    'vMiN': 'yes',\n",
    "    'vMP': 'yes',\n",
    "    'vMT': 'yes',\n",
    "    'vMTmi': 'no',\n",
    "    'vMTmu': 'no',\n",
    "    'vNMu': 'yes',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig. 1: Performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_classes_for_summary = ['MT', 'MTx', 'MTo', 'MTox',\n",
    "                           'MP', 'MPx', 'MPo', 'MPox',\n",
    "                           'DS', 'DSx', 'DSo', 'DSox', \n",
    "                           'AC', 'ACx', 'ACo', 'ACox',\n",
    "                           'DA', 'DAx', 'DAo', 'DAox',\n",
    "]\n",
    "\n",
    "# Function to calculate the correct response percentage\n",
    "def correct_response_percentage(row):\n",
    "    correct_answer = correct_answers[row['prompt_class']]\n",
    "    return abs(row[f'{correct_answer}_percent']) * 100\n",
    "\n",
    "# Applying the function to each row\n",
    "df['correct_response_percentage'] = df.apply(correct_response_percentage, axis=1)\n",
    "\n",
    "# Include only the selected prompt classes\n",
    "filtered_df = df[df['prompt_class'].isin(prompt_classes_for_summary)]\n",
    "\n",
    "# Calculating the average correct response percentage for each model\n",
    "mean_correct_response = filtered_df.groupby('model').correct_response_percentage.mean().reset_index()\n",
    "\n",
    "# Sorting the models by average correct response percentage in descending order\n",
    "mean_correct_response = mean_correct_response.sort_values('correct_response_percentage', ascending=False)\n",
    "\n",
    "# Adjusting color mapping bounds\n",
    "min_performance = mean_correct_response['correct_response_percentage'].min()\n",
    "max_performance = mean_correct_response['correct_response_percentage'].max()\n",
    "adjusted_lower_bound = 0.1\n",
    "more_adjusted_upper_bound = 0.325\n",
    "mean_correct_response['more_fine_tuned_normalized_performance'] = mean_correct_response['correct_response_percentage'].apply(\n",
    "    lambda x: min(max((x - min_performance) / (max_performance - min_performance) * more_adjusted_upper_bound + adjusted_lower_bound, 0), 1))\n",
    "# Mapping each model's performance to a color in the more fine-tuned gist_rainbow palette\n",
    "mean_correct_response['more_fine_tuned_color'] = mean_correct_response['more_fine_tuned_normalized_performance'].apply(lambda x: plt.cm.gist_rainbow(x))\n",
    "\n",
    "# Plotting the graph\n",
    "plt.figure(figsize=(9,6))\n",
    "sns.barplot(x='correct_response_percentage', y='model', data=mean_correct_response, palette=mean_correct_response['more_fine_tuned_color'])\n",
    "plt.xlabel('Average correct answer frequency', fontsize=16)\n",
    "plt.ylabel('', fontsize=14)\n",
    "plt.title('Summary of performance on some simple inferences', fontsize=16)\n",
    "plt.xticks([0, 25, 50, 75, 100], ['0%', '25%', '50%', '75%', '100%'], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlim(0, 100)\n",
    "plt.tight_layout()\n",
    "\n",
    "if not os.path.exists('graphs'):\n",
    "    os.makedirs('graphs')\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "pdf_file_path = 'graphs/summary_graph_multicolor.pdf'\n",
    "plt.savefig(pdf_file_path, format='pdf')\n",
    "\n",
    "#plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top of Fig. 2: Queries and Errors per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the heatmap\n",
    "heatmap_data = pd.DataFrame({\n",
    "    'Queries': queries_per_model,\n",
    "    'Both Responses Errors': error_both_response,\n",
    "    'Neither Response Errors': error_no_response\n",
    "})\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Create a mask for the Queries per Model column\n",
    "mask = np.zeros_like(heatmap_data, dtype=bool)\n",
    "mask[:, 0] = True\n",
    "\n",
    "# Use rc_context to set font size locally\n",
    "with plt.rc_context({'font.size': 12}):\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\"d\", cmap='Reds', mask=mask, cbar_kws={'label': 'Error Count'}, cbar=False)\n",
    "    plt.title(\"Queries and Errors per Model\",fontsize=16)\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    # Manually annotate the 'Queries per Model' column\n",
    "    for i, value in enumerate(heatmap_data['Queries']):\n",
    "        plt.text(0.5, i + 0.5, value, horizontalalignment='center', verticalalignment='center', color='black')\n",
    "\n",
    "    # Remove ticks on the x and y axes\n",
    "    plt.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False)\n",
    "\n",
    "    # Save the plot as a PDF file with a tight layout\n",
    "    plt.savefig('results/queries_and_errors.pdf', bbox_inches='tight')\n",
    "\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom of Fig. 2: Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_classes_for_correlations = ['AC', 'DA', 'DS', 'DSmi', 'DSmu', 'MP', 'MT', 'MTmi', 'MTmu', 'MiN', \n",
    "                                   'ACx', 'DAx', 'DSx', 'DSmix', 'DSmux', 'MPx', 'MTx', 'MTmix', 'MTmux', 'MiNx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate average 'yes_percent' for 'x' and non-'x' versions across models and temperatures\n",
    "def calculate_averages_by_temp(df):\n",
    "    averages_dict = {}\n",
    "    for prompt_class in set(pc.rstrip('x') for pc in df['prompt_class']):\n",
    "        prompt_class_x = prompt_class + 'x'\n",
    "        for model in df['model'].unique():\n",
    "            for temp in df['temperature'].unique():\n",
    "                avg_non_x = df[(df['prompt_class'] == prompt_class) & (df['model'] == model) & (df['temperature'] == temp)]['yes_percent'].mean()\n",
    "                avg_x = df[(df['prompt_class'] == prompt_class_x) & (df['model'] == model) & (df['temperature'] == temp)]['yes_percent'].mean()\n",
    "                averages_dict[(prompt_class, model, temp)] = {'avg_non_x': avg_non_x, 'avg_x': avg_x}\n",
    "    return averages_dict\n",
    "\n",
    "# Function to calculate correlation across models by temperature for each prompt class\n",
    "def calculate_correlation_across_models_by_temp(averages_dict):\n",
    "    correlation_dict = {}\n",
    "    for (prompt_class, model, temp) in averages_dict:\n",
    "        key = (prompt_class, temp)\n",
    "        avg_non_x = averages_dict[(prompt_class, model, temp)]['avg_non_x']\n",
    "        avg_x = averages_dict[(prompt_class, model, temp)]['avg_x']\n",
    "        if key not in correlation_dict:\n",
    "            correlation_dict[key] = {'avg_non_x': [], 'avg_x': []}\n",
    "        correlation_dict[key]['avg_non_x'].append(avg_non_x)\n",
    "        correlation_dict[key]['avg_x'].append(avg_x)\n",
    "    for key in correlation_dict:\n",
    "        series_non_x = pd.Series(correlation_dict[key]['avg_non_x'])\n",
    "        series_x = pd.Series(correlation_dict[key]['avg_x'])\n",
    "        correlation = series_non_x.corr(series_x)\n",
    "        correlation_dict[key] = correlation\n",
    "    return correlation_dict\n",
    "\n",
    "# Function to calculate general correlation across all models and temperatures for each prompt class\n",
    "def calculate_general_correlation_across_all_models_and_temps(averages_dict):\n",
    "    correlation_dict = {}\n",
    "    for key in averages_dict:\n",
    "        prompt_class = key[0]\n",
    "        avg_non_x = averages_dict[key]['avg_non_x']\n",
    "        avg_x = averages_dict[key]['avg_x']\n",
    "        if prompt_class not in correlation_dict:\n",
    "            correlation_dict[prompt_class] = {'avg_non_x': [], 'avg_x': []}\n",
    "        correlation_dict[prompt_class]['avg_non_x'].append(avg_non_x)\n",
    "        correlation_dict[prompt_class]['avg_x'].append(avg_x)\n",
    "    for prompt_class in correlation_dict:\n",
    "        series_non_x = pd.Series(correlation_dict[prompt_class]['avg_non_x'])\n",
    "        series_x = pd.Series(correlation_dict[prompt_class]['avg_x'])\n",
    "        correlation = series_non_x.corr(series_x)\n",
    "        correlation_dict[prompt_class] = correlation\n",
    "    return correlation_dict\n",
    "\n",
    "# Function to calculate general correlation across all prompt classes, models, and temperatures\n",
    "def calculate_general_correlation_across_prompt_classes_models_and_temps(averages_dict):\n",
    "    all_non_x_values = []\n",
    "    all_x_values = []\n",
    "    for key in averages_dict:\n",
    "        all_non_x_values.append(averages_dict[key]['avg_non_x'])\n",
    "        all_x_values.append(averages_dict[key]['avg_x'])        \n",
    "    general_correlation = pd.Series(all_non_x_values).corr(pd.Series(all_x_values))\n",
    "    return general_correlation\n",
    "\n",
    "\n",
    "# Perform the analysis\n",
    "\n",
    "filtered_df = df[df['prompt_class'].isin(prompt_classes_for_correlations)]\n",
    "\n",
    "averages_dict_by_temp = calculate_averages_by_temp(filtered_df)\n",
    "correlation_across_models_by_temp = calculate_correlation_across_models_by_temp(averages_dict_by_temp)\n",
    "general_correlation_across_all_models_and_temps = calculate_general_correlation_across_all_models_and_temps(averages_dict_by_temp)\n",
    "general_correlation_across_prompt_classes_models_and_temps = calculate_general_correlation_across_prompt_classes_models_and_temps(averages_dict_by_temp)\n",
    "\n",
    "# Print the results\n",
    "print(\"Correlation Across Models by Temperature:\")\n",
    "for key, value in correlation_across_models_by_temp.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"\\nGeneral Correlation Across All Models and Temperatures:\")\n",
    "for key, value in general_correlation_across_all_models_and_temps.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"\\nGeneral Correlation Across All Prompt Classes, Models, and Temperatures:\")\n",
    "print(general_correlation_across_prompt_classes_models_and_temps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique temperatures and prompt classes\n",
    "unique_temperatures = sorted(set(temp for _, temp in correlation_across_models_by_temp.keys()))\n",
    "unique_prompt_classes = sorted(set(prompt_class for prompt_class, _ in correlation_across_models_by_temp.keys()))\n",
    "\n",
    "# Convert the dictionary to a format suitable for a DataFrame\n",
    "data_for_heatmap = {temp: {prompt_class: correlation_across_models_by_temp.get((prompt_class, temp), None)\n",
    "                           for prompt_class in unique_prompt_classes}\n",
    "                    for temp in unique_temperatures}\n",
    "\n",
    "# Convert to DataFrame and drop rows with all NaN values\n",
    "correlation_df = pd.DataFrame(data_for_heatmap).dropna(how='all')\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Use rc_context to set font size locally\n",
    "with plt.rc_context({'font.size': 12}):\n",
    "    sns.heatmap(correlation_df, annot=True, cmap='RdYlGn', center=0, fmt=\".2f\", cbar=False)\n",
    "    plt.title(\"Correlations between frequencies of affirmative response \\nto prompts with sensical vs. nonsensical predicates\")\n",
    "    plt.ylabel(\"Prompt Class\")\n",
    "    plt.xlabel(\"Temperature\")\n",
    "\n",
    "    # Save the plot as a PDF file with a tight layout\n",
    "    plt.savefig('results/correlations.pdf', bbox_inches='tight')\n",
    "\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures with green bars: average frequency of correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in temperatures:\n",
    "\n",
    "    # Filter the dataframe for selected temperature\n",
    "    df_filtered = df[df['temperature'] == temp]\n",
    "\n",
    "    unique_prompt_classes = df_filtered['prompt_class'].unique()\n",
    "\n",
    "    model_order = [\"GPT-4 (0613)\", \"GPT-3.5 Turbo (0613)\", \"Llama 2 Chat 70B\", \"Mixtral 8x7B\",\n",
    "                \"Yi Chat 34B\", \"Code Llama 34B\", \"Code Llama 13B\",\n",
    "                \"Llama 2 Chat 13B\", \"Mistral 7B\", \"Code Llama 7B\",\n",
    "                \"Llama 2 Chat 7B\", \"Phi-2\"]\n",
    "    \n",
    "    model_shortened_names = {name:name for name in model_order}\n",
    "    model_shortened_names[\"GPT-4 (0613)\"] = \"GPT-4\"\n",
    "    model_shortened_names[\"GPT-3.5 Turbo (0613)\"] = \"GPT-3.5 Turbo\"\n",
    "\n",
    "    # Colors from the seaborn \"bright\" palette\n",
    "    bright_palette = sns.color_palette(\"bright\")\n",
    "    bright_color = bright_palette[2] # Choosing a color for the correct answer\n",
    "\n",
    "    # Dictionary to store file paths for each prompt's graph\n",
    "    percent_graph_paths = {}\n",
    "\n",
    "    for prompt_class in tqdm(unique_prompt_classes):\n",
    "        # Filtering for the current prompt class\n",
    "        prompt_class_df = df_filtered[df_filtered['prompt_class'] == prompt_class].copy()\n",
    "        prompt_class_df['model'] = pd.Categorical(prompt_class_df['model'], categories=model_order, ordered=True)\n",
    "        prompt_class_df['model'] = prompt_class_df['model'].replace(model_shortened_names)\n",
    "\n",
    "        # Select the correct percentage based on the correct answer\n",
    "        correct_percent_column = 'yes_percent' if correct_answers[prompt_class] == 'yes' else 'no_percent'\n",
    "        prompt_class_df[correct_percent_column] = abs(prompt_class_df[correct_percent_column]) * 100  # Converting to positive percentage\n",
    "\n",
    "        # Creating a multi-line title\n",
    "        prompt = prompt_class +  \" example: \" + prompt_class_df['prompt'].iloc[0]\n",
    "        words = prompt.split()\n",
    "        title_lines = []\n",
    "        line = \"\"\n",
    "        for word in words:\n",
    "            if len(line) + len(word) <= 55:\n",
    "                line += word + \" \"\n",
    "            else:\n",
    "                title_lines.append(line.strip())\n",
    "                line = word + \" \"\n",
    "        title_lines.append(line.strip()) # Append the last line\n",
    "        title = \"\\n\".join(title_lines)\n",
    "\n",
    "        # Creating the plot\n",
    "        plt.figure(figsize=(9, 6))\n",
    "        ax = plt.gca()\n",
    "        plt.title(title, fontsize=16)\n",
    "\n",
    "        # Plotting bars\n",
    "        sns.barplot(x=correct_percent_column, y=\"model\", data=prompt_class_df, color=bright_color, edgecolor='black')\n",
    "\n",
    "        # Extending the x-axis range\n",
    "        plt.xlabel(f'Correct answer (\\'{correct_answers[prompt_class]}\\') frequency', fontsize='16')\n",
    "        plt.ylabel('')\n",
    "        plt.xlim(0, 105) # Extending x-axis to 110% for visual clarity\n",
    "\n",
    "        # Adjusting the x-axis ticks and labels\n",
    "        ax.set_xticks([0, 25, 50, 75, 100])\n",
    "        ax.set_xticklabels(['0%', '25%', '50%', '75%', '100%'])\n",
    "\n",
    "        plt.yticks(fontsize='16') \n",
    "        plt.xticks(fontsize='16') \n",
    "\n",
    "        ax.legend([], [], frameon=False)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot\n",
    "        output_file = f'graphs/correct_freq_for_{prompt_class}_temp_{temp}.pdf'\n",
    "        plt.savefig(output_file, format='pdf')\n",
    "        plt.close()\n",
    "\n",
    "        # Updating the dictionary with the new file paths\n",
    "        percent_graph_paths[prompt_class] = output_file\n",
    "\n",
    "    # Output the paths of the saved graphs with extended x-axis\n",
    "    print(percent_graph_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures with orange bars: average frequency of 'no' and average frequency of 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in temperatures:\n",
    "\n",
    "    # Filter the dataframe for the selected temperature\n",
    "    df_filtered = df[df['temperature'] == temp]\n",
    "\n",
    "    unique_prompt_classes = df_filtered['prompt_class'].unique()\n",
    "\n",
    "    model_order = [\"GPT-4 (0613)\", \"GPT-3.5 Turbo (0613)\", \"Llama 2 Chat 70B\", \"Mixtral 8x7B\",\n",
    "                \"Yi Chat 34B\", \"Code Llama 34B\", \"Code Llama 13B\",\n",
    "                \"Llama 2 Chat 13B\", \"Mistral 7B\", \"Code Llama 7B\",\n",
    "                \"Llama 2 Chat 7B\", \"Phi-2\"]\n",
    "    \n",
    "    model_shortened_names = {name:name for name in model_order}\n",
    "    model_shortened_names[\"GPT-4 (0613)\"] = \"GPT-4\"\n",
    "    model_shortened_names[\"GPT-3.5 Turbo (0613)\"] = \"GPT-3.5 Turbo\"\n",
    "\n",
    "    # Colors from the seaborn \"bright\" palette\n",
    "    bright_palette = sns.color_palette(\"bright\")\n",
    "    bright_orange = bright_palette[1]\n",
    "\n",
    "    # Dictionary to store file paths for each prompt's graph\n",
    "    percent_graph_paths = {}\n",
    "\n",
    "    for prompt_class in tqdm(unique_prompt_classes):\n",
    "        # Filtering for the current prompt class\n",
    "        prompt_class_df = df_filtered[df_filtered['prompt_class'] == prompt_class].copy()\n",
    "        prompt_class_df['model'] = pd.Categorical(prompt_class_df['model'], categories=model_order, ordered=True)\n",
    "        prompt_class_df['model'] = prompt_class_df['model'].replace(model_shortened_names)\n",
    "\n",
    "        # Converting frequencies to percentages\n",
    "        prompt_class_df['yes_percent'] *= 100\n",
    "        prompt_class_df['no_percent'] *= 100\n",
    "\n",
    "        # Creating a multi-line title\n",
    "        prompt = prompt_class + \" example: \" + prompt_class_df['prompt'].iloc[0]\n",
    "        words = prompt.split()\n",
    "        title_lines = []\n",
    "        line = \"\"\n",
    "        for word in words:\n",
    "            if len(line) + len(word) <= 60:\n",
    "                line += word + \" \"\n",
    "            else:\n",
    "                title_lines.append(line.strip())\n",
    "                line = word + \" \"\n",
    "        title_lines.append(line.strip())  # Append the last line\n",
    "        title = \"\\n\".join(title_lines)\n",
    "\n",
    "        # Creating the plot\n",
    "        plt.figure(figsize=(8.5, 6))\n",
    "        ax = plt.gca()\n",
    "        plt.title(title, fontsize=16)\n",
    "\n",
    "        # Plotting bars\n",
    "        sns.barplot(x=\"yes_percent\", y=\"model\", data=prompt_class_df, color=bright_orange, edgecolor='black')\n",
    "        sns.barplot(x=\"no_percent\", y=\"model\", data=prompt_class_df, color=bright_orange, edgecolor='black')\n",
    "\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        plt.xlim(-105, 105)\n",
    "        plt.axvline(x=0, color='black', linewidth=1)\n",
    "\n",
    "        # Adding text\n",
    "        font_size = plt.rcParams['axes.labelsize']\n",
    "        ax.text(50, -0.07, \"'Yes' Frequency\", ha=\"center\", va=\"top\", transform=ax.get_xaxis_transform(), fontsize=16)\n",
    "        ax.text(-50, -0.07, \"'No' Frequency\", ha=\"center\", va=\"top\", transform=ax.get_xaxis_transform(), fontsize=16)\n",
    "\n",
    "        ax.set_xticks([-100, -50, 0, 50, 100])\n",
    "        ax.set_xticklabels(['100%', '50%', '0', '50%', '100%'])\n",
    "\n",
    "        ax.legend([], [], frameon=False)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Adjust subplots if necessary (modify these values as needed)\n",
    "        plt.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.9, hspace=0.2, wspace=0.2)\n",
    "\n",
    "        plt.yticks(fontsize='16') \n",
    "        plt.xticks(fontsize='16') \n",
    "\n",
    "        # Save the plot\n",
    "        output_file = f'graphs/percent_graph_for_{prompt_class}_temp_{temp}.pdf'\n",
    "        plt.savefig(output_file, format='pdf', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Updating the dictionary with the new file paths\n",
    "        percent_graph_paths[prompt_class] = output_file\n",
    "\n",
    "        #plt.show()\n",
    "\n",
    "    # Output the paths of the saved graphs\n",
    "    percent_graph_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Further analysis mentioned in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_correlation(prompt_class1,prompt_class2):\n",
    "\n",
    "    prompt_class1_freq = []\n",
    "    prompt_class2_freq = []\n",
    "\n",
    "    for model in df['model'].unique():\n",
    "        for temp in df['temperature'].unique():\n",
    "            prompt_class1_mean = df[(df['prompt_class'] == prompt_class1) & (df['model'] == model) & (df['temperature'] == temp)]['yes_percent'].mean()\n",
    "            prompt_class2_mean = df[(df['prompt_class'] == prompt_class2) & (df['model'] == model) & (df['temperature'] == temp)]['yes_percent'].mean() \n",
    "\n",
    "            if np.isnan(prompt_class1_mean):\n",
    "                print(df[(df['prompt_class'] == prompt_class1) & (df['model'] == model) & (df['temperature'] == temp)]['yes_percent'])\n",
    "\n",
    "            prompt_class1_freq.append(prompt_class1_mean)\n",
    "            prompt_class2_freq.append(prompt_class2_mean)\n",
    "\n",
    "    correlation = pd.Series(prompt_class1_freq).corr(pd.Series(prompt_class2_freq))\n",
    "\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned in the text, the correlation is very high between responses to 'AC' and responses to variants\n",
    "# that replace 'infer' with 'conclude', 'deduce', 'logically infer', 'logically conclude', etc.\n",
    "\n",
    "for prompt in ['cAC', 'dAC', 'gAC', 'gnAC', 'LAC', 'LcAC', 'LdAC', 'sAC', 'snAC']:\n",
    "\n",
    "    print(f\"Correlation between AC and {prompt}: {yes_correlation('AC',prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned in the text, the correlation is somewhat lower between the 'v' versions and regular versions of prompts:\n",
    "\n",
    "base_prompts = ['MP', 'MT', 'AC', 'DA', 'DS','MTmi', 'MTmu', 'DSmi', 'DSmu']\n",
    "\n",
    "for prompt in base_prompts:\n",
    "    print(f\"Correlation between {prompt} and {prompt}x: {yes_correlation(prompt,prompt+'x')}\")\n",
    "    print(f\"Correlation between {prompt} and v{prompt}: {yes_correlation(prompt,'v'+prompt)}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
